{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Variational Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T07:12:41.467512Z",
     "start_time": "2019-02-09T07:12:41.463283Z"
    }
   },
   "source": [
    "See:\n",
    "* [Great first resource](http://www.cmap.polytechnique.fr/~zoltan.szabo/jc/2017_05_18_Massil_Achab.pdf)\n",
    "* [VI: A review for statisticians](https://arxiv.org/pdf/1601.00670.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T06:54:36.772536Z",
     "start_time": "2019-02-09T06:54:36.452505Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Kullbackâ€“Leibler (KL) Divergence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The KL divergence measures how similar two probability distributions are. For discrete PDFs this is:\n",
    "\n",
    "$$\n",
    "D_{KL}(P || Q) = \\sum_{x \\in X} P(x) \\log \\big(\\frac{P(x)}{Q(x)}\\big)\n",
    "$$\n",
    "\n",
    "The divergence is measured in bits of information if the log used is base 2, and nats if it is base $e$. It can be understood as the amount of information gained when moving from $P \\rightarrow Q$, or as the amount of information lost when approximating $P$ as $Q$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-02-09T07:15:22.064477Z",
     "start_time": "2019-02-09T07:15:22.047888Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.15415067982725839 0.14291239755557528\n",
      "0.0\n"
     ]
    }
   ],
   "source": [
    "def discrete_KL_divergence(p, q):\n",
    "    assert np.isclose(np.sum(p), 1) and np.isclose(np.sum(q), 1) and len(p) == len(q)\n",
    "    return np.sum(p * np.log(p / q))\n",
    "\n",
    "p = np.array([2, 2, 2])\n",
    "q = np.array([1, 4, 2])\n",
    "p, q = p / np.sum(p), q / np.sum(q)\n",
    "\n",
    "print(discrete_KL_divergence(p, q), discrete_KL_divergence(q, p))\n",
    "\n",
    "print(discrete_KL_divergence(p, p))\n",
    "\n",
    "# As always if you actually want to use this, use scipy's.\n",
    "assert np.isclose(scipy.stats.entropy(q, p), discrete_KL_divergence(q, p))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Some interesting properties:\n",
    "* $D_{KL} = 0$ iff $Q(x) = P(x)$ for all $x$ (as $\\log(1) = 0$).\n",
    "* $D_{KL} >= 0$ see [Gibb's inequality](https://en.wikipedia.org/wiki/Gibbs%27_inequality)\n",
    "* It is not symmetric! Be careful that this isn't the divergence *between* $P$ and $Q$ but rather the divergence *from* $P$ to $Q$.\n",
    "* It is undefined if $Q(x) = 0$ where $P(x) \\neq 0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Variational Inference Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "The short summary of VI is:\n",
    "\n",
    "* We have some unknown and complicated probability function $P$.\n",
    "* Propose a family of easier to use distributions $D$.\n",
    "* Find some $Q \\in D$ that is a good approximation to $P$ by minimizing $D_{KL}(P || Q)$. \n",
    "* Use $Q$ to do whatever we were going to do with $P$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## A Reminder About Bayesian Inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "In Bayesian inference we have some observed variables $x$ and some hidden or [latent](https://en.wikipedia.org/wiki/Latent_variable) variables $z$ that encode the structure behind $x$. E.g. is cosmology $x$ are your observations (galaxies, CMB, etc) and $z$ are the cosmological parameters ($\\Omega_M$, $\\sigma_8$, etc). We want to constrain $z$ using the data $x$.\n",
    "\n",
    "[Bayes Theorem](https://en.wikipedia.org/wiki/Bayes%27_theorem) tells us that:\n",
    "\n",
    "$$\n",
    "P(z | x) = \\frac{P(z) P(x | z)}{P(x)}\n",
    "$$\n",
    "\n",
    "or in english, the posterior probability of the parameters $z$ given the data $x$ is equal to the prior on $z$ multiplied by the likelihood of that data given those parameters. This is normalized by the evidence $P(x)$. Remember, we want to learn the posterior $P(z | x)$.\n",
    "\n",
    "We can remove the conditionals, $P(a | b)$, and instead express the posterior as a function of joint, $P(a, b)$, distributions.\n",
    "\n",
    "$$\n",
    "P(z | x) = \\frac{P(x, z)}{\\int P(x, z)\\ dz}\n",
    "$$\n",
    "\n",
    "Computing this in practice is hard because we would need to do that integral over the potentially high dimensional $z$ space. We need to do something different."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Evidence Lower Bound (ELBO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Assume we have some proposed $Q(z)$. We want to see how similar this is to the posterior $P(z | x)$. **N.N.** these are both defined on $z$ not $x$!\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\ \\ \\ D_{KL}(Q(z) || P(z | x)) \\\\\n",
    "&= \\int Q(z) \\log \\big(\\frac{Q(z)}{P(z | x)}\\big) dz \\\\\n",
    "&= \\int Q(z) ( \\log(Q(z)) - \\log(P(z | x)) ) dz \\\\\n",
    "&= \\int Q(z) \\log(Q(z)) dz - \\int Q(z) \\log(P(z | x)) dz\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "These transformations are all good but we are still stuck with the posterior which we can't compute... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Let's make the notation a bit more concise using expected values. Remember:\n",
    "\n",
    "$$\n",
    "\\mathbf{E}^P[X] = \\int x P(x) dx \\\\\n",
    "\\mathbf{E}^P[g(X)] = \\int g(x) P(x) dx\n",
    "$$\n",
    "\n",
    "we say, the expectation value of $g(x)$ with the probability function $P$ is... So similarly,\n",
    "\n",
    "$$\n",
    "\\int Q(z) \\log(P(z | x)) = \\mathbf{E}^Q[\\log(P(z | x)]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "With this notation,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\ \\ \\ D_{KL}(Q(z) || P(z | x)) \\\\\n",
    "&= \\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(P(z | x))] \\\\\n",
    "&= \\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(\\frac{P(x, z)}{P(x)})] \\\\\n",
    "&= \\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(P(x, z)) - \\log(P(x))] \\\\\n",
    "&= \\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(P(x, z))] + \\mathbf{E}^Q[\\log(P(x))]\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "but this final term is just $\\int Q(z) \\log(P(x)) dz = \\log(P(x)) \\int Q(z) dz = \\log(P(x))$ and so,\n",
    "\n",
    "$$\n",
    "D_{KL}(Q(z) || P(z | x)) = \\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(P(x, z))] + \\log(P(x))\n",
    "$$\n",
    "\n",
    "We've now got rid of the posterior and instead have a joint and the evidence. The joint can be expressed as a prior and a likelihood $P(x, z) = P(z) P(x | z)$ which is easy to compute. The evidence is still a problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We want to minimize the KL divergence. However, as we have just seen this has a constant term related to the evidence. We can't compute this term (remember the reason we are going through all this complicated stuff is because we can't compute this) but this doesn't matter. As it is a constant we can just minimize, $\\mathbf{E}^Q[\\log(Q(z))] - \\mathbf{E}^Q[\\log(P(x, z))]$. In practice we tend to maximize the negative of this which we call ELBO.\n",
    "\n",
    "$$\n",
    "{\\rm ELBO}(Q) = \\mathbf{E}^Q[\\log(P(x, z))] - \\mathbf{E}^Q[\\log(Q(z))]\n",
    "$$\n",
    "\n",
    "maximizing this ELBO function is equivalent to minimizing the KL divergence.\n",
    "\n",
    "The name, Evidence Lower Bound, comes from the fact that ELBO is always less than or equal to the log evidence ($P(x)$). It is a lower bound on the evidence.\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "D_{KL}(Q(z) || P(z | x)) &= -{\\rm ELBO}(Q) + \\log(P(x)) \\\\\n",
    "\\log(P(x)) &= D_{KL}(Q(z) || P(z | x)) + {\\rm ELBO}(Q) \\\\\n",
    "\\therefore \\log(P(x)) &>= {\\rm ELBO}(Q)\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "this inequality is because the KL divergence is always non-negative. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## Family of Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "We said that we were drawing $Q(z)$ from some family of distributions $D$. We wan't this $Q$ to be easy to work with. A popular choice is the **mean-field variational family**.\n",
    "\n",
    "I don't understand this yet..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This follows section 3 in [VI: A review for statisticians](https://arxiv.org/pdf/1601.00670.pdf)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "learning",
   "language": "python",
   "name": "learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
